{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import numbers\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import PIL.Image as Image\n",
    "import pickle\n",
    "\n",
    "###########\n",
    "# from   utils import CONFIG\n",
    "global cfg\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Convert ndarrays in sample to Tensors with normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, phase=\"test\"):\n",
    "        global cfg\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        self.phase = phase\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # convert GBR images to RGB\n",
    "        image, alpha, trimap = sample['image'][:,:,::-1], sample['alpha'], sample['trimap']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = torch.from_numpy(image.transpose((2, 0, 1)).astype('float32')) / 255.\n",
    "        alpha = torch.from_numpy(alpha.astype('float32')).unsqueeze(dim=0)\n",
    "\n",
    "        # normalize image\n",
    "        mask = np.equal(trimap, 128).astype('float32')\n",
    "        mask = torch.from_numpy(mask).unsqueeze(dim=0)\n",
    "        trimap = torch.from_numpy(trimap.astype('float32')).unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "        if self.phase == \"train\":\n",
    "            # convert GBR images to RGB\n",
    "            fg = torch.from_numpy(sample['fg'][:,:,::-1].transpose((2, 0, 1)).astype('float32')) / 255.\n",
    "            bg = torch.from_numpy(sample['bg'][:,:,::-1].transpose((2, 0, 1)).astype('float32')) / 255.\n",
    "            alpha = torch.cat((alpha, mask, fg, bg, image), dim=0)\n",
    "        else:\n",
    "            alpha = torch.cat((alpha, trimap), dim=0)\n",
    "        image = (image - self.mean) / self.std\n",
    "        image = torch.cat((image, trimap/255.), dim=0)\n",
    "\n",
    "        sample['image'], sample['alpha'] = image, alpha\n",
    "\n",
    "\n",
    "        if self.phase == \"train\":\n",
    "            del sample['image_name']\n",
    "            del sample['trimap']\n",
    "            del sample['fg']\n",
    "            del sample['bg']\n",
    "        else:\n",
    "            del sample['trimap']\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class RandomAffine(object):\n",
    "    \"\"\"\n",
    "    Random affine translation\n",
    "    \"\"\"\n",
    "    def __init__(self, degrees, translate=None, scale=None, shear=None, flip=None, resample=False, fillcolor=0):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            assert isinstance(degrees, (tuple, list)) and len(degrees) == 2, \\\n",
    "                \"degrees should be a list or tuple and it must be of length 2.\"\n",
    "            self.degrees = degrees\n",
    "\n",
    "        if translate is not None:\n",
    "            assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n",
    "                \"translate should be a list or tuple and it must be of length 2.\"\n",
    "            for t in translate:\n",
    "                if not (0.0 <= t <= 1.0):\n",
    "                    raise ValueError(\"translation values should be between 0 and 1\")\n",
    "        self.translate = translate\n",
    "\n",
    "        if scale is not None:\n",
    "            assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n",
    "                \"scale should be a list or tuple and it must be of length 2.\"\n",
    "            for s in scale:\n",
    "                if s <= 0:\n",
    "                    raise ValueError(\"scale values should be positive\")\n",
    "        self.scale = scale\n",
    "\n",
    "        if shear is not None:\n",
    "            if isinstance(shear, numbers.Number):\n",
    "                if shear < 0:\n",
    "                    raise ValueError(\"If shear is a single number, it must be positive.\")\n",
    "                self.shear = (-shear, shear)\n",
    "            else:\n",
    "                assert isinstance(shear, (tuple, list)) and len(shear) == 2, \\\n",
    "                    \"shear should be a list or tuple and it must be of length 2.\"\n",
    "                self.shear = shear\n",
    "        else:\n",
    "            self.shear = shear\n",
    "\n",
    "        self.resample = resample\n",
    "        self.fillcolor = fillcolor\n",
    "        self.flip = flip\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees, translate, scale_ranges, shears, flip, img_size):\n",
    "        \"\"\"Get parameters for affine transformation\n",
    "        Returns:\n",
    "            sequence: params to be passed to the affine transformation\n",
    "        \"\"\"\n",
    "        angle = random.uniform(degrees[0], degrees[1])\n",
    "        if translate is not None:\n",
    "            max_dx = translate[0] * img_size[0]\n",
    "            max_dy = translate[1] * img_size[1]\n",
    "            translations = (np.round(random.uniform(-max_dx, max_dx)),\n",
    "                            np.round(random.uniform(-max_dy, max_dy)))\n",
    "        else:\n",
    "            translations = (0, 0)\n",
    "\n",
    "        if scale_ranges is not None:\n",
    "            scale = (random.uniform(scale_ranges[0], scale_ranges[1]),\n",
    "                     random.uniform(scale_ranges[0], scale_ranges[1]))\n",
    "        else:\n",
    "            scale = (1.0, 1.0)\n",
    "\n",
    "        if shears is not None:\n",
    "            shear = random.uniform(shears[0], shears[1])\n",
    "        else:\n",
    "            shear = 0.0\n",
    "\n",
    "        if flip is not None:\n",
    "            # flip = (np.random.rand(2) < flip).astype(np.int) * 2 - 1\n",
    "            flip = (np.random.rand(2) < flip).astype(np.uint8) * 2 - 1\n",
    "\n",
    "        return angle, translations, scale, shear, flip\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        fg, alpha = sample['fg'], sample['alpha']\n",
    "        rows, cols, ch = fg.shape\n",
    "        if np.maximum(rows, cols) < 1024:\n",
    "            params = self.get_params((0, 0), self.translate, self.scale, self.shear, self.flip, fg.size)\n",
    "        else:\n",
    "            params = self.get_params(self.degrees, self.translate, self.scale, self.shear, self.flip, fg.size)\n",
    "\n",
    "        center = (cols * 0.5 + 0.5, rows * 0.5 + 0.5)\n",
    "        M = self._get_inverse_affine_matrix(center, *params)\n",
    "        M = np.array(M).reshape((2, 3))\n",
    "\n",
    "        fg = cv2.warpAffine(fg, M, (cols, rows),\n",
    "                            flags=cv2.INTER_NEAREST + cv2.WARP_INVERSE_MAP)\n",
    "        alpha = cv2.warpAffine(alpha, M, (cols, rows),\n",
    "                               flags=cv2.INTER_NEAREST + cv2.WARP_INVERSE_MAP)\n",
    "\n",
    "        sample['fg'], sample['alpha'] = fg, alpha\n",
    "\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    @ staticmethod\n",
    "    def _get_inverse_affine_matrix(center, angle, translate, scale, shear, flip):\n",
    "        # Helper method to compute inverse matrix for affine transformation\n",
    "\n",
    "        # As it is explained in PIL.Image.rotate\n",
    "        # We need compute INVERSE of affine transformation matrix: M = T * C * RSS * C^-1\n",
    "        # where T is translation matrix: [1, 0, tx | 0, 1, ty | 0, 0, 1]\n",
    "        # C is translation matrix to keep center: [1, 0, cx | 0, 1, cy | 0, 0, 1]\n",
    "        # RSS is rotation with scale and shear matrix\n",
    "        # It is different from the original function in torchvision\n",
    "        # The order are changed to flip -> scale -> rotation -> shear\n",
    "        # x and y have different scale factors\n",
    "        # RSS(shear, a, scale, f) = [ cos(a + shear)*scale_x*f -sin(a + shear)*scale_y     0]\n",
    "        # [ sin(a)*scale_x*f          cos(a)*scale_y             0]\n",
    "        # [     0                       0                      1]\n",
    "        # Thus, the inverse is M^-1 = C * RSS^-1 * C^-1 * T^-1\n",
    "\n",
    "        angle = math.radians(angle)\n",
    "        shear = math.radians(shear)\n",
    "        scale_x = 1.0 / scale[0] * flip[0]\n",
    "        scale_y = 1.0 / scale[1] * flip[1]\n",
    "\n",
    "        # Inverted rotation matrix with scale and shear\n",
    "        d = math.cos(angle + shear) * math.cos(angle) + math.sin(angle + shear) * math.sin(angle)\n",
    "        matrix = [\n",
    "            math.cos(angle) * scale_x, math.sin(angle + shear) * scale_x, 0,\n",
    "            -math.sin(angle) * scale_y, math.cos(angle + shear) * scale_y, 0\n",
    "        ]\n",
    "        matrix = [m / d for m in matrix]\n",
    "\n",
    "        # Apply inverse of translation and of center translation: RSS^-1 * C^-1 * T^-1\n",
    "        matrix[2] += matrix[0] * (-center[0] - translate[0]) + matrix[1] * (-center[1] - translate[1])\n",
    "        matrix[5] += matrix[3] * (-center[0] - translate[0]) + matrix[4] * (-center[1] - translate[1])\n",
    "\n",
    "        # Apply center translation: C * RSS^-1 * C^-1 * T^-1\n",
    "        matrix[2] += center[0]\n",
    "        matrix[5] += center[1]\n",
    "\n",
    "        return matrix\n",
    "\n",
    "class RandomJitter(object):\n",
    "    \"\"\"\n",
    "    Random change the hue of the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        fg, alpha = sample['fg'], sample['alpha']\n",
    "        # if alpha is all 0 skip\n",
    "        if np.all(alpha==0):\n",
    "            return sample\n",
    "        # convert to HSV space, convert to float32 image to keep precision during space conversion.\n",
    "        fg = cv2.cvtColor(fg.astype(np.float32)/255.0, cv2.COLOR_BGR2HSV)\n",
    "        # Hue noise\n",
    "        hue_jitter = np.random.randint(-40, 40)\n",
    "        fg[:, :, 0] = np.remainder(fg[:, :, 0].astype(np.float32) + hue_jitter, 360)\n",
    "        # Saturation noise\n",
    "        sat_bar = fg[:, :, 1][alpha > 0].mean()\n",
    "        sat_jitter = np.random.rand()*(1.1 - sat_bar)/5 - (1.1 - sat_bar) / 10\n",
    "        sat = fg[:, :, 1]\n",
    "        sat = np.abs(sat + sat_jitter)\n",
    "        sat[sat>1] = 2 - sat[sat>1]\n",
    "        fg[:, :, 1] = sat\n",
    "        # Value noise\n",
    "        val_bar = fg[:, :, 2][alpha > 0].mean()\n",
    "        val_jitter = np.random.rand()*(1.1 - val_bar)/5-(1.1 - val_bar) / 10\n",
    "        val = fg[:, :, 2]\n",
    "        val = np.abs(val + val_jitter)\n",
    "        val[val>1] = 2 - val[val>1]\n",
    "        fg[:, :, 2] = val\n",
    "        # convert back to BGR space\n",
    "        fg = cv2.cvtColor(fg, cv2.COLOR_HSV2BGR)\n",
    "        sample['fg'] = fg*255\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"\n",
    "    Random flip image and label horizontally\n",
    "    \"\"\"\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "    def __call__(self, sample):\n",
    "        fg, alpha = sample['fg'], sample['alpha']\n",
    "        if np.random.uniform(0, 1) < self.prob:\n",
    "            fg = cv2.flip(fg, 1)\n",
    "            alpha = cv2.flip(alpha, 1)\n",
    "        sample['fg'], sample['alpha'] = fg, alpha\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"\n",
    "    Crop randomly the image in a sample, retain the center 1/4 images, and resize to 'output_size'\n",
    "    :param output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "        self.margin = output_size[0] // 2\n",
    "        self.logger = logging.getLogger(\"Logger\")\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        fg, alpha, trimap, name = sample['fg'], sample['alpha'], sample['trimap'], sample['image_name']\n",
    "        bg = sample['bg']\n",
    "        h, w = trimap.shape\n",
    "        hbg, wbg = bg.shape[:2]\n",
    "        ratio = max(h/hbg, w/wbg)\n",
    "        if ratio > 1:\n",
    "            bg = cv2.resize(bg, (math.ceil(wbg*ratio), math.ceil(hbg*ratio)), interpolation=cv2.INTER_CUBIC)\n",
    "            hbg, wbg = bg.shape[:2]\n",
    "        if h < self.output_size[0] + 1 or w < self.output_size[1] + 1:\n",
    "            ratio = 1.1 * self.output_size[0] / h if h < w else 1.1 * self.output_size[1] / w\n",
    "            # self.logger.warning(\"Size of {} is {}.\".format(name, (h, w)))\n",
    "            while h < self.output_size[0] + 1 or w < self.output_size[1] + 1:\n",
    "                fg = cv2.resize(fg, (int(w * ratio), int(h * ratio)),\n",
    "                                interpolation=cv2.INTER_NEAREST)\n",
    "                alpha = cv2.resize(alpha, (int(w * ratio), int(h * ratio)),\n",
    "                                   interpolation=cv2.INTER_NEAREST)\n",
    "                trimap = cv2.resize(trimap, (int(w * ratio), int(h * ratio)), interpolation=cv2.INTER_NEAREST)\n",
    "                bg = cv2.resize(bg, (int(wbg * ratio), int(hbg * ratio)),\n",
    "                                interpolation=cv2.INTER_CUBIC)\n",
    "                h, w = trimap.shape\n",
    "        unknown_list = list(zip(*np.where(trimap[self.margin:(h - self.margin),\n",
    "                                          self.margin:(w - self.margin)] == 128)))\n",
    "\n",
    "        unknown_num = len(unknown_list)\n",
    "        if len(unknown_list) < 10:\n",
    "            # self.logger.warning(\"{} does not have enough unknown area for crop.\".format(name))\n",
    "            left_top = (\n",
    "            np.random.randint(0, h - self.output_size[0] + 1), np.random.randint(0, w - self.output_size[1] + 1))\n",
    "        else:\n",
    "            idx = np.random.randint(unknown_num)\n",
    "            left_top = (unknown_list[idx][0], unknown_list[idx][1])\n",
    "\n",
    "        fg_crop = fg[left_top[0]:left_top[0] + self.output_size[0], left_top[1]:left_top[1] + self.output_size[1]]\n",
    "        alpha_crop = alpha[left_top[0]:left_top[0] + self.output_size[0], left_top[1]:left_top[1] + self.output_size[1]]\n",
    "        bg_crop = bg[left_top[0]:left_top[0] + self.output_size[0], left_top[1]:left_top[1] + self.output_size[1]]\n",
    "        trimap_crop = trimap[left_top[0]:left_top[0] + self.output_size[0],\n",
    "                      left_top[1]:left_top[1] + self.output_size[1]]\n",
    "\n",
    "        if len(np.where(trimap == 128)[0]) == 0:\n",
    "            self.logger.error(\"{} does not have enough unknown area for crop. Resized to target size.\"\n",
    "                              \"left_top: {}\".format(name, left_top))\n",
    "            fg_crop = cv2.resize(fg, self.output_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
    "            alpha_crop = cv2.resize(alpha, self.output_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
    "            trimap_crop = cv2.resize(trimap, self.output_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
    "            bg_crop = cv2.resize(bg, self.output_size[::-1], interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        sample['fg'], sample['alpha'], sample['trimap'] = fg_crop, alpha_crop, trimap_crop\n",
    "        sample['bg'] = bg_crop\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class GenTrimap(object):\n",
    "    def __init__(self):\n",
    "        self.erosion_kernels = [None] + [cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size)) for size in range(1,30)]\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        alpha = sample['alpha']\n",
    "        # Adobe 1K\n",
    "        # fg_width = np.random.randint(1, 30)\n",
    "        # bg_width = np.random.randint(1, 30)\n",
    "        fg_width = np.random.randint(1, 10)\n",
    "        bg_width = np.random.randint(1, 10)\n",
    "        fg_mask = (alpha + 1e-5).astype(np.int).astype(np.uint8)\n",
    "        bg_mask = (1 - alpha + 1e-5).astype(np.int_).astype(np.uint8)\n",
    "        fg_mask = cv2.erode(fg_mask, self.erosion_kernels[fg_width])\n",
    "        bg_mask = cv2.erode(bg_mask, self.erosion_kernels[bg_width])\n",
    "\n",
    "        trimap = np.ones_like(alpha) * 128\n",
    "        trimap[fg_mask == 1] = 255\n",
    "        trimap[bg_mask == 1] = 0\n",
    "\n",
    "        sample['trimap'] = trimap\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Composite(object):\n",
    "    def __call__(self, sample):\n",
    "        fg, bg, alpha = sample['fg'], sample['bg'], sample['alpha']\n",
    "        alpha[alpha < 0 ] = 0\n",
    "        alpha[alpha > 1] = 1\n",
    "        fg[fg < 0 ] = 0\n",
    "        fg[fg > 255] = 255\n",
    "        bg[bg < 0 ] = 0\n",
    "        bg[bg > 255] = 255\n",
    "\n",
    "        image = fg * alpha[:, :, None] + bg * (1 - alpha[:, :, None])\n",
    "        sample['image'] = image\n",
    "\n",
    "        return sample\n",
    "    \n",
    "###############################################################\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self, Cfg, phase=\"train\", test_scale=\"resize\", crop_size = 320, augmentation=True):\n",
    "        '''\n",
    "        cfg: config文件\n",
    "        phase: string. 默认train,当前data generator状态，是training还是testing\n",
    "        crop_size: int. 默认320，提取config文件中训练数据的裁剪大小\n",
    "        augmentation: boolean. 默认True，是否需要data augmentation操作\n",
    "        \n",
    "        fg: numpy array. 保存了所有foreground图片的绝对路径\n",
    "        alpha： numpy array. 保存了所有alpha图片的绝对路径\n",
    "        bg： numpy array. 保存了所有background图片的绝对路径\n",
    "        fg_num: int. foreground图片个数\n",
    "        bg_num: int. background图片个数\n",
    "        \n",
    "        fg_load: dict. key是foreground图片名，value是图片的numpy.ndarray\n",
    "        bg_load: dict. key是background图片名，value是图片的numpy.ndarray\n",
    "        alpha_load: dict. key是alpha图片名，value是图片的numpy.ndarray\n",
    "        \n",
    "        transform: dict. key是train, val, test，value是一个list，里面是一系列的transform操作. 根据不同的phase，选择不同的transform操作\n",
    "        '''\n",
    "        \n",
    "        global cfg\n",
    "        cfg = Cfg\n",
    "        self.phase = phase #当前data generator状态，是training还是testing\n",
    "        self.crop_size = cfg.TRAIN.crop_size #提取config文件中训练数据的裁剪大小\n",
    "        self.augmentation = augmentation #是否需要data augmentation操作\n",
    "        if self.phase == \"train\": #如果当前状态是training\n",
    "            self.fg = np.array([os.path.join(cfg.DATASET.data_dir, name) for name in\n",
    "                       open(cfg.DATASET.train_fg_list).read().splitlines()]) \n",
    "            #cfg.DATASET.train_fg_list是train_fg.txt文件，合并foreground图片文件的绝对路径为一个大list，并且转换为numpy array\n",
    "            \n",
    "            self.alpha = np.array([os.path.join(cfg.DATASET.data_dir, name) for name in\n",
    "                          open(cfg.DATASET.train_alpha_list).read().splitlines()])\n",
    "            #cfg.DATASET.train_alpha_list是train_alpha.txt文件，合并alpha图片文件的绝对路径为一个大list，并且转换为numpy array\n",
    "            \n",
    "            \n",
    "            self.bg = np.array([os.path.join(cfg.DATASET.data_dir, name) for name in \n",
    "                                open(cfg.DATASET.train_bg_list).read().splitlines()])\n",
    "            #同理\n",
    "            \n",
    "            self.bg_num = len(self.bg)\n",
    "            self.fg_num = len(self.fg)\n",
    "            #background图片和foreground图片的数量\n",
    "            \n",
    "            if cfg.TRAIN.load_data: #在config文件的training部分：whether to load fg, alpha to memory。默认为false\n",
    "                self.fg_load = dict()\n",
    "                self.alpha_load = dict()\n",
    "                #fg_load和alpha_load是两个字典，key是image名字，value是foreground value和alpha value\n",
    "                \n",
    "                for idx in range(self.fg_num):\n",
    "                    fg_name = self.fg[idx] #第idx位置的foreground image路径\n",
    "                    fg_name = fg_name[fg_name.rfind('/') + 1:-4] #foreground image文件名， xxxx.jpg，xxxx的部分\n",
    "                    fg = cv2.imread(self.fg[idx]) #读取foreground image 默认彩色 (numpy.ndarray)\n",
    "                    alpha = cv2.imread(self.alpha[idx], 0).astype(np.float32) / 255. #读取alpha image 默认灰度 (numpy.float32)\n",
    "                    # 读取alpha，0相当于cv2.IMREAD_GRAYSCALE。\n",
    "                    # 并且将alpha从[0, 255]区间映射为[0, 1]区间，为了便于分析\n",
    "                    \n",
    "                    self.fg_load.update({fg_name: fg})\n",
    "                    self.alpha_load.update({fg_name: alpha})\n",
    "                    # 将foreground value和alpha value （[0, 1]）保存到fg_load和alpha_load两个字典中,字典参考前面的注释\n",
    "                    \n",
    "            if cfg.TRAIN.load_bg: #whether to load bg to memory，默认为false，以下操作和alpha foreground一致\n",
    "                self.bg_load = dict()\n",
    "                for idx in range(self.bg_num):\n",
    "                    bg_name = self.bg[idx] # 加载background,和alpha, foreground同理\n",
    "                    bg_name = bg_name[bg_name.rfind('/')+1:-4]\n",
    "                    bg = cv2.imread(self.bg[idx], 1)\n",
    "                    self.bg_load.update({bg_name: bg})\n",
    "                    \n",
    "        else: #如果当前状态不是training:\n",
    "            self.test_list = np.array([name.split('\\t') for name in open(cfg.DATASET.val_list).read().splitlines()])\n",
    "            # cfg.DATASET.val_list指向的是test.txt文件 \n",
    "            # 里面一行有三个图片路径并且用tab分隔开，他们是merged, alpha和trimaps\n",
    "\n",
    "        if augmentation: #如果需要data augumentation操作\n",
    "            train_trans = [\n",
    "                            RandomAffine(degrees=30, scale=[0.8, 1.25], shear=10, flip=0.5), #图片保持重心不变的图像随机仿射变换\n",
    "                            \n",
    "                            RandomHorizontalFlip(), #图片在给定几率下随机进行水平翻转，默认为0.5\n",
    "                            \n",
    "                            GenTrimap(),\n",
    "                            \n",
    "                            RandomCrop((self.crop_size, self.crop_size)), #图片随机裁剪一块图像出来, output size是 (self.crop_size, self.crop_size)\n",
    "                            \n",
    "                            RandomJitter(), #随机抖动\n",
    "                            \n",
    "                            Composite(), #根据foreground, alpha, background合成图像\n",
    "                            \n",
    "                            ToTensor(phase=\"train\") \n",
    "                            ] #Convert ndarrays in sample to Tensors with normalization\n",
    "        else:\n",
    "            train_trans = [ GenTrimap(),\n",
    "                            RandomCrop((self.crop_size, self.crop_size)), #图片随机裁剪一块图像出来, output size是 (self.crop_size, self.crop_size)\n",
    "                            Composite(),\n",
    "                            ToTensor(phase=\"train\") ]\n",
    "\n",
    "        if test_scale.lower() == \"origin\":\n",
    "            #如果不需要做改变，就只转换成tensor\n",
    "            test_trans = [ToTensor()]\n",
    "        else:\n",
    "            raise NotImplementedError(\"test_scale {} not implemented\".format(test_scale))\n",
    "\n",
    "        self.transform = {\n",
    "            # transform是一个字典，key是train, val, test，value是一个list，里面是一系列的transform操作，根据不同的phase选择不同的transform操作\n",
    "            'train':\n",
    "                transforms.Compose(train_trans),\n",
    "            'val':\n",
    "                transforms.Compose([\n",
    "                    ToTensor()\n",
    "                ]),\n",
    "            'test':\n",
    "                transforms.Compose(test_trans)\n",
    "        }[phase]\n",
    "\n",
    "        self.erosion_kernels = [None] + [cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size)) for size in range(1,20)]\n",
    "        # 生成一个椭圆形的核，用于腐蚀(erosion)操作，腐蚀操作是一种图像处理的操作，用于去除小的噪声点，或者连接两个分开的物体\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根据index，找到一个名为sample的index，这个sample是一个字典，根据不同的phase，sample里面的key和value是不一样的\n",
    "        # 然后根据不同的phase，对sample里面的value进行不同的transform操作\n",
    "        # transform操作是在__init__里面定义的\n",
    "        \n",
    "        if self.phase == \"train\":\n",
    "            if not cfg.TRAIN.load_data:\n",
    "                # load foreground\n",
    "                fg = cv2.imread(self.fg[idx % self.fg_num]) #源代码\n",
    "                \n",
    "                # load alpha\n",
    "                \n",
    "                alpha = cv2.imread(self.alpha[idx % self.fg_num], 0).astype(np.float32) / 255 # 源代码\n",
    "                \n",
    "                # alpha = cv2.imread(self.alpha[idx], 0).astype(np.float32) / 255 # 修正代码\n",
    "            else:\n",
    "                # load foreground\n",
    "                fg_name = self.fg[idx % self.fg_num]\n",
    "                fg_name = fg_name[fg_name.rfind('/')+1:-4]\n",
    "\n",
    "                fg = self.fg_load[fg_name]\n",
    "                # fg是fg_load字典里面的一个key(fg_name)所对应的value是foreground的图片（numpy.ndarray）\n",
    "                alpha = self.alpha_load[fg_name]\n",
    "                # alpha是alpha_load字典里面的一个key(fg_name)所对应的value是alpha的图片（numpy.float32）\n",
    "                \n",
    "            bg_idx = np.random.randint(0, self.bg_num - 1) if cfg.TRAIN.random_bgidx else idx #随机选择一个background图片的index\n",
    "            if not cfg.TRAIN.load_bg: #如果没load background\n",
    "                bg = cv2.imread(self.bg[bg_idx], 1) #读取background图片\n",
    "            else:\n",
    "                bg_name = self.bg[idx % self.bg_num] #源代码    \n",
    "                bg_name = bg_name[bg_name.rfind('/') + 1:-4] # bg_name是background图片的名字\n",
    "                # print(bg_name)\n",
    "                bg = self.bg_load[bg_name] # bg是bg_load字典里面的一个key(bg_name)所对应的value是background的图片（numpy.ndarray）\n",
    "                \n",
    "            if bg.shape[2]==1:  # 如果background图片是灰度图，\n",
    "                bg = np.repeat(bg, 3, axis=2) #如果background图片是灰度图，就把它变成RGB图\n",
    "                \n",
    "            alpha = np.squeeze(alpha) #去掉alpha中的维度为1的维度\n",
    "\n",
    "            if self.augmentation: #如果需要data augmentation\n",
    "                fg, alpha = self._composite_fg(fg, alpha, idx)\n",
    "\n",
    "            image_name = os.path.split(self.fg[idx % self.fg_num])[-1]\n",
    "            sample = {'fg': fg, 'alpha': alpha, 'bg': bg, 'image_name': image_name} #把foreground （np.array）, alpha（np.array）, background（np.array）, image_name(string)放到一个字典（sample）里面\n",
    "\n",
    "        else: #如果当前是test阶段\n",
    "            image = cv2.imread(os.path.join(cfg.DATASET.data_dir, self.test_list[idx][0])) #读取image\n",
    "            alpha = cv2.imread(os.path.join(cfg.DATASET.data_dir, self.test_list[idx][1]), 0).astype(np.float32) / 255. #读取alpha\n",
    "            alpha = alpha[:, :, 0] if alpha.ndim == 3 else alpha #如果alpha是3维的，就把它变成2维的\n",
    "            trimap = cv2.imread(os.path.join(cfg.DATASET.data_dir, self.test_list[idx][2]), 0) #读取trimap\n",
    "            image_name = os.path.split(self.test_list[idx][0])[-1] #image_name是image的名字\n",
    "\n",
    "            sample = {'image': image, 'alpha': alpha, 'trimap': trimap} #sample是一个字典，里面包含image, alpha, trimap\n",
    "\n",
    "        sample = self.transform(sample) #根据当前phase，对sçample进行对应的transform操作\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _composite_fg(self, fg, alpha, idx): #合并foreground和alpha\n",
    "\n",
    "        if np.random.rand() < 0.5: \n",
    "            idx2 = np.random.randint(self.fg_num) + idx #随机选择一个foreground图片的index\n",
    "            if not cfg.TRAIN.load_data: # 如果不load fg, alpha to memory\n",
    "                fg2 = cv2.imread(self.fg[idx2 % self.fg_num]) #随机选择一个foreground图片作为fg2\n",
    "                alpha2 = cv2.imread(self.alpha[idx2 % self.fg_num], 0).astype(np.float32) / 255. #随机选择一个alpha图片作为alpha2\n",
    "            else:\n",
    "                fg2_name = self.fg[idx2 % self.fg_num] \n",
    "                fg2_name = fg2_name[fg2_name.rfind('/') + 1:-4] # fg2_name是随机选择的foreground图片的名字\n",
    "                fg2 = self.fg_load[fg2_name] \n",
    "                alpha2 = self.alpha_load[fg2_name].astype(np.float32) / 255.\n",
    "              \n",
    "            # resizes fg2 and alpha2 to the same size as fg and alpha \n",
    "            alpha2 = np.squeeze(alpha2)\n",
    "            h, w = alpha.shape\n",
    "            fg2 = cv2.resize(fg2, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            alpha2 = cv2.resize(alpha2, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "\n",
    "            alpha_tmp = 1 - (1 - alpha) * (1 - alpha2) #合并alpha和alpha2\n",
    "            if np.any(alpha_tmp < 1): #如果合并后的alpha_tmp中有小于1的值\n",
    "                fg = ((fg.astype(np.float32) * alpha[:, :, None] + fg2.astype(np.float32) * (1 - alpha[:, :, None]) * alpha2[:, :, None])) \\\n",
    "                     / (alpha_tmp[:, :, None] + 1e-5) #合并fg和fg2\n",
    "                # The overlap of two 50% transparency should be 25%\n",
    "                alpha = alpha_tmp\n",
    "                fg = fg.astype(np.uint8)\n",
    "\n",
    "        if np.random.rand() < 0.25: \n",
    "            #把fg和alpha都resize到640*640, 并且使用最近邻插值\n",
    "            fg = cv2.resize(fg, (640, 640), interpolation=cv2.INTER_NEAREST) \n",
    "            alpha = cv2.resize(alpha, (640, 640), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        return fg, alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.phase == \"train\":\n",
    "            return self.bg_num\n",
    "        else:\n",
    "            return len(self.test_list)\n",
    "\n",
    "#####################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "995\n",
      "1000\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_396809/172179627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/data_generator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mbg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbg_num\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_bgidx\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0midx\u001b[0m \u001b[0;31m#随机选择一个background图片的index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_bg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#如果没load background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 \u001b[0mbg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbg_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#读取background图片\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0mbg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbg_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    from data_generator import DataGenerator\n",
    "    from torch.utils.data import DataLoader\n",
    "    from config import cfg\n",
    "\n",
    "    # logging.basicConfig(level=logging.DEBUG, format='[%(asctime)s] %(levelname)s: %(message)s', datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "    cfg.merge_from_file('/home/xiufeng/Code/config/aiml.yaml')\n",
    "\n",
    "    dataset = DataGenerator(cfg, phase='train', test_scale='origin', crop_size=cfg.TRAIN.crop_size)\n",
    "    dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            drop_last=True,\n",
    "            sampler=None\n",
    "        )\n",
    "\n",
    "    print(len(dataloader))\n",
    "    print(dataset.fg_num)\n",
    "    print(dataset.bg_num)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize = (8, 8))\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        images, targets = data['image'], data['alpha']\n",
    "        print(i)\n",
    "        # print(images.shape)\n",
    "        # print(targets.shape)\n",
    "        # images = images.numpy().reshape(512, 512, -1)\n",
    "        # images = images[:, :, 3]\n",
    "        # plt.subplot(2, 2, 1)\n",
    "        # plt.imshow(images)\n",
    "        \n",
    "        # targets = targets.numpy().reshape(512, 512, -1)\n",
    "        # targets1 = targets[:, :, 0]\n",
    "        # plt.subplot(2, 2, 2)\n",
    "        # plt.imshow(targets1)\n",
    "        \n",
    "        # targets2 = targets[:, :, 5:8]\n",
    "        # plt.subplot(2, 2, 3)\n",
    "        # plt.imshow(targets2)\n",
    "        \n",
    "        # targets3 = targets[:, :, 8:11]\n",
    "        # plt.subplot(2, 2, 4)\n",
    "        # plt.imshow(targets3)\n",
    "        \n",
    "        \n",
    "        # alpha = targets[0,0].numpy()\n",
    "        # img = Image.fromarray(alpha)\n",
    "        # img.show()\n",
    "        # plt.show()       \n",
    "        # break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1624c84e230f9acbbc73e7e331a37ed5444cce9bf59d0e543ceaf376aa4bf3ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
